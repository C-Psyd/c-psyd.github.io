---
title: "C.Psyd - Schedule"
layout: textlay
excerpt: "C.Psyd -- Schedule"
sitemap: false
permalink: /schedule/
---

# Schedule

C.Psyd meetings consist of paper presentations, project workshopping, invited speakers, brainstorming sessions, etc. The format frequently changes between terms. If you'd like to join C.Psyd meetings, please send me an email.

# Invited Speakers



**2020**
: **Stefan Frank** (Radboud University)
: _Neural models of bilingual sentence processing_
: A bilingual's two grammars do not form independent systems but interact with each other, as is clear from phenomena such as syntactic transfer, cross-linguistic structural priming, and code-switching. I will present neural network models of bilingual sentence processing that show how (some of) these phenomena can emerge from mere exposure to two languages, that is, without requiring cognitive mechanisms that are specific to bilingualism.  
:
: **Aurelie Herbelot** (University of Trento)
: _Modelling the acquisition of linguistic competences from small data_
: There is currently much optimism in the field of Natural
Language Processing (NLP): some basic linguistic tasks are considered
'solved', while others have tremendously benefited from the introduction
of novel neural architectures. However, the data, training regimes and
system architectures required to obtain top performance are often
unrealistic from the point of view of human cognition. It is therefore
questionable whether current NLP systems can ever earn the name of
'models' of language learning. In this talk, we will subject well-known
algorithms to one specific constraint on human acquisition: limited
input. The first part of the talk will focus on RNN architectures and
analyse their level of grammatical competence when trained over 3
million tokens from child-directed language. The second part will
investigate the issue of semantic competence, looking at the behaviour
of word embedding systems with respect to three aspects of meaning:
lexical knowledge, reference, distributional properties. We will
conclude that NLP systems can actually adapt well to small data, but
that their success may be highly dependent on the nature of the data
they receive, as well as the underlying representations they learn from. (Work with Ludovica Pannitto)  
:
: **Maayan Keshev** (Tel-Aviv University)
: _Noisy is better than rare: Evidence from processing ambiguous relative clauses in Hebrew_
: During sentence processing readers may utilize their top-down knowledge to overcome possible noise in their input. Thus, the interpretation of improbable strings could be pulled towards a likely near-neighbour. In the current study, I exhibit this kind of rational noisy-channel inference in processing of Hebrew relative clauses which are ambiguous between SR and OR readings. I suggest that readers may be willing to compromise agreement information in order to construct a SR, depending on the prior probability of the OR structure. Thus, a corrupted SR (with mismatching verbal agreement) is preferred over a grammatical OR with a rare word order. Yet, readers opt for the OR parse if it is not extremely rare (though presumably less frequent than the SR structure).

